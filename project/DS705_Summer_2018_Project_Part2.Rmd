---
title: 'Predicting Loan Defaults with Logistic Regression'
author: "Matt Allen"
date: "07/29/2018"
output: word_document
fontsize: 12pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
require(ggplot2)
require(grid)
require(gridExtra)
```

## 1. Executive Summary
## 2. Introduction

The purpose of this project is to build a model to predict whether or not a borrower will pay back a loan. The description of the data set is [here](https://datascienceuwl.github.io/Project2018/loans50k.csv). The model will give a simple "Good" or "Bad" indicating the quality of the loan. The project will begin by preparing and cleaning the data. Variables will be selected by domain knowledge versus an automated method. Through exploration of the selected variables, the variables may be transformed to satisfy conditions of model fitting like being normally distributed for example. The data will be fitted to a logistic model with a training data set. Part of the data set will be held out as test data set to validate the model. The model is binomial and the mean also called the classification threshold can be adjusted to optimize for accuracy or profit.  

## 3. Preparing and Cleaning the Data

The first step in preparing the data was to create a new column named response that has the values "Good" and "Bad". It was based on the variable status. Only the rows with status of "Fully Paid", "Charged off", and "Default" were kept. "Fully Paid" maps to "Good", and "Charged off" and "Default" were mapped to "Bad".

```{r create_response_column, include=FALSE}

filePath <- "C:/Users/matt/source/repos/ds705/project/loans50k.csv"
#filePath <- "C:/Users/irgepi/source/repos/ds705/project/loans50k.csv"
loans <- read.csv2(filePath, header = TRUE, sep = ",")
loans$response <- loans$status
#summary(loans$verified)

#Good loans are those that have a status of "Fully Paid". Bad loans are those that have a status of "Charged Off" or "Default".
levels(loans$response)[levels(loans$response)==c("Charged Off")] <- "Bad"
levels(loans$response)[levels(loans$response)==c("Default")] <- "Bad"
levels(loans$response)[levels(loans$response)==c("Fully Paid")] <- "Good"
loans <- loans[which(loans$response=="Bad" | loans$response=="Good"), ]

#remove unuused levels
loans$response <- factor(loans$response)



#transform Source Verified to Verified
levels(loans$verified)[levels(loans$verified)==c("Source Verified")] <- "Verified"
#remove unuused levels
loans$verified <- factor(loans$verified)

```


The initial dataset consisted of 50,000 records. After reducing "Good" and "Bad", the total number of records left were 34,655 records. The number of records with the variable response equal to "Good" was 27,074, and "Bad" was 7,581.

There were variables that could be removed, because they were not useful. For example, the status variable may be removed, because it has been transformed into the response variable. The totalPaid variable can be removed, because it is not knowable at the time of issue of a loan. The variable loanid was removed because it is just an identifier and has no predictive power. 

The variable employment, which indicates job title, can be removed because there is so much variation. It would be possible to create a new variable with job title that puts titles in more general categories, but it probably is still not useful. A better indicator may be income from job. 

The variable length indicates the length of employment. This could be a good indicator, but there are more than 1300 missing values. It should be removed, because removing the rows with missing income would significantly reduce the data set, and imputation methods may be challenging for this much missing data where there is not necessarily a good proxy.

The variable revolRatio indicates proportion of revolving credit in use. This can be removed, because it is captured in other variables like debtIncRatio. With similar resoning, bcRatio can be removed. The variable accOpen24 can be removed, because if an account is opened there will likely be a credit inquiry, which is captured in inq6mth.

The variable grade indicates the risk of the loan. I would consider it to be another response variable like status or the response variable derived from status, but with more levels. For this reason, it should be eliminated. The variable rate can be removed for similar reasons as grade. The loan rate is an indication of the borrower's risk. The greater the risk the higher the rate. 

The variable pubRec can be eliminated, because the variable delinq2yr should capture this information. If someone has pubRec against them, they would have also missed payments. 

After inspection of the histograms of the variables totalBal and totalLim, it was discovered that their distributions are bimodal, which violates condition of normality, so debtIncRatio was used as a proxy. The variable payment was found to be strongly correlated with amount, and was removed. The variable openAcc was found not to be significant and was removed.

After elimination of variables, the predictors that are left are amount, verified, income, debtIncRat, delinq2yr and inq6mth. The variable verified is categorical, the rest are quantitative. 

With the remaining variables, there is no imputation or record removal needed. 

```{r reduce_variables, include=FALSE}
  #eliminate length, many missing values what is another a proxy? show count of n/a.
  table(loans$length)
  class(loans$length)
  #1823 as n/a
  length(which(loans$length == "n/a"))

  columns_to_keep <- c("response","totalPaid", "amount", "debtIncRat","verified","income", "delinq2yr","inq6mth")
  loans <- loans[columns_to_keep]

  #convert income, amount, debtIncRat and totalPaid to numeric
  loans$income <- as.numeric(as.character(loans$income))
  loans$amount <- as.numeric(as.character(loans$amount))
  loans$totalPaid <- as.numeric(as.character(loans$totalPaid))
  loans$debtIncRat <- as.numeric(as.character(loans$debtIncRat))
```


## 4. Exploring and Transforming the Data
#log_amount + log_income + home + debtIncRat + cube_rt_delinq2yr + cube_rt_inq6mth
The variables amount, income, delinq2yr, inq6mth showed varying degrees of right skewness. From inspection of histograms, the variable amount and income showed the most right skewness and were transformed by the log function. The variables delinq2yr and inq6mth were transformed by taking the cubed root. Histograms of the transformed variables are displayed in Figure 1.


```{r transform_and_plot_variables, echo=FALSE}
#transform variables
loans$log_amount <- log(loans$amount)
loans$log_income <- log(loans$income)
loans$cube_rt_delinq2yr <- (loans$delinq2yr)^(1/3)
loans$cube_rt_inq6mth <- (loans$inq6mth)^(1/3)
#https://stats.stackexchange.com/questions/11406/boxplot-with-respect-to-two-factors-using-ggplot2-in-r

log_amount_plot <- ggplot(loans, aes(x=log_amount)) + 
  geom_histogram(binwidth=.1, colour="black", fill="white")

log_income_plot <- ggplot(loans, aes(x=log_income)) + 
  geom_histogram(binwidth=.1, colour="black", fill="white")

cube_rt_delinq2yr_plot <- ggplot(loans, aes(x=cube_rt_delinq2yr)) + 
  geom_histogram(binwidth=1, colour="black", fill="white")

cube_rt_inq6mth_plot <- ggplot(loans, aes(x=cube_rt_inq6mth)) + 
  geom_histogram(binwidth=1, colour="black", fill="white")

debtIncRat_plot <- ggplot(loans, aes(x=debtIncRat)) + 
  geom_histogram(binwidth=1, colour="black", fill="white")


#https://cran.r-project.org/web/packages/gridExtra/vignettes/arrangeGrob.html

grid.arrange(log_amount_plot, log_income_plot, cube_rt_delinq2yr_plot, cube_rt_inq6mth_plot, debtIncRat_plot, nrow=3, ncol=2, top="Predictors", bottom = "Figure 1 Histograms of Numeric Predictors")
```


To start to develop a picture of the relationships between the predictor variables, and the response, two boxplots were created for a predictor variable. The boxplots were created to explore the differences in distributions between "Good" and "Bad" loans for a predictor variable. The plots can be seen in figure 2. Looking at the log_amount variable box plot, the mean of amount is lower for Good than Bad, which makes sense. A smaller loan would be easier to pay back. Also as indicated by log_income, those with greater incomes have more money to pay back a loan. 

```{r explore_relationships, echo=FALSE}
log_amount_boxplot <- ggplot(aes(y = log_amount, x = response), data = loans) + geom_boxplot()
log_income_boxplot <- ggplot(aes(y = log_income, x = response), data = loans) + geom_boxplot()

cube_rt_delinq2yr_boxplot <- ggplot(aes(y = cube_rt_delinq2yr, x = response), data = loans) + geom_boxplot()
cube_rt_inq6mth_boxplot <- ggplot(aes(y = cube_rt_inq6mth, x = response), data = loans) + geom_boxplot()

debtIncRat_boxplot <- ggplot(aes(y = debtIncRat, x = response), data = loans) + geom_boxplot()

grid.arrange(log_amount_boxplot, log_income_boxplot, cube_rt_delinq2yr_boxplot, cube_rt_inq6mth_boxplot, debtIncRat_boxplot, nrow=3, ncol=2, top="Predictors by Response", bottom = "Figure 2 Boxplots of Numeric Predictors by Response")
```

After inspection of the boxplot cube_rt_delinq2yr, the variable delinq2yr was removed from the model, because even after transformation it is extremely right skewed. Taking it out of the model did not effect the accuracy or profit predictions significantly.


## 5. The Logistic Model

```{r create_training_test_datasets, include=FALSE}
#https://stackoverflow.com/questions/17200114/how-to-split-data-into-training-testing-sets-using-sample-function
## 80% of the sample size
smp_size <- floor(0.8 * nrow(loans))

## set the seed to make your partition reproducible
set.seed(123)
train_ind <- sample(seq_len(nrow(loans)), size = smp_size)

train <- loans[train_ind, ]
test <- loans[-train_ind, ]
```

```{r create_model, include=FALSE}
#cube_rt_delinq2yr +
loan.model <- glm(formula = response ~ log_amount + log_income + verified + debtIncRat + cube_rt_inq6mth, family = "binomial", data = train)
extractAIC(loan.model)
summary(loan.model)
summary(test)
```

```{r vif, include=FALSE}
require(HH)
vif(response ~ log_amount + log_income + debtIncRat + cube_rt_delinq2yr + cube_rt_inq6mth,  data = train)
```


```{r contigency_table, echo=FALSE}
#predprob <- fitted(loan.model) # get predicted probabilities
predprob <- predict(loan.model, test, type="response")
threshhold <- 0.5  # Set Y=1 when predicted probability exceeds this
predLoan <- cut(predprob, breaks=c(-Inf, threshhold, Inf), 
                labels=c("Bad", "Good"))  

cTab <- table(test$response, predLoan) 
addmargins(cTab)

p <- sum(diag(cTab)) / sum(cTab)  # compute the proportion of correct classifications
print(paste('Proportion correctly predicted = ', p)) 
```

## 6. Optimizing the Threshold for Accuracy
```{r accuracy, echo=FALSE}
model.accuracy <- function(threshhold) {
  predprob <- predict(loan.model, test, type="response")
  predLoan <- cut(predprob, breaks=c(-Inf, threshhold, Inf), 
                  labels=c("Bad", "Good"))  

  cTab <- table(test$response, predLoan) 
  addmargins(cTab)

  p <- sum(diag(cTab)) / sum(cTab)  # compute the proportion of correct classifications
}

threshholds <- seq(from = 0, to = 1, length.out = 30)
accuracy <- sapply(threshholds,model.accuracy)
accuracy.df <- data.frame(threshhold=threshholds, accuracy=accuracy)
#model is same accuracy from 0 to .5.
plot(accuracy~threshholds)
```
## 7. Optimizing the Threshold for Profit

```{r profit, echo=FALSE}
model.profit <- function(threshhold) {
  predprob <- predict(loan.model, test, type="response")
  profit.df <- test[which(predprob > threshhold),]
  profit <- sum(profit.df$totalPaid - profit.df$amount)
}
threshholds <- seq(from = 0, to = 1, length.out = 30)
profit <- sapply(threshholds,model.profit)

threshholds[which.max(profit)] #threshold of max profit
plot(profit~threshholds)
```

## 8. Results Summary
