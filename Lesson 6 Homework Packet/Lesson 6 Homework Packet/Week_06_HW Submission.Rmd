
---
title: 'Simple and Multiple Linear Regression'
author: "Matt Allen"
date: "07/04/2018"
output: word_document
fontsize: 12pt
---

Knit a Word file from this R Markdown file for the following exercises.  Submit the R markdown file and resulting Word file via D2L Dropbox.   

## Exercise 1

The data for this problem comes from a dataset presented in Mackowiak, P. A., Wasserman, S. S., and Levine, M. M.  (1992), "A Critical Appraisal of 98.6 Degrees F, the Upper Limit of the Normal Body Temperature, and Other Legacies of Carl Reinhold August Wunderlich," Journal of the American Medical Association, 268, 1578-1580.  Body temperature (in degrees Fahrenheit) and heart rate (in beats per minute) were two variables that were measured for a random sample of 130 adults.  A simple linear regression was used to see if body temperature had an effect on heart rate.

The data are in the file normtemp.rda in the DS705data package, this data is included in the DS705data package so you can access it by loading the package and typing data(normtemp).

### Part 1a

Create a scatterplot with heart rate in the vertical axis and plot the estimated linear regression line in the scatterplot. Include descriptive labels for the x and y-axes (not just the variable names as they are in the data file). 

Note:  this data set needs a little cleaning first.  The heart rates are missing for two of the rows.  You can delete these rows from the data frame using the R function na.omit().  Just put the name of the data frame in the parenthesis.

Does it appear that a linear model is at least possibly a plausible model for the relationship between heart rate and body temperature?  Explain your answer.

### -|-|-|-|-|-|-|-|-|-|-|- Answer 1a -|-|-|-|-|-|-|-|-|-|-|-

```{r}
# Get data
require(DS705data)
data("normtemp")
# Remove Missing Values
normtemp <- na.omit(normtemp)
loess_line <- loess(hr~temp,normtemp)
{
  # Create Scatter Plot
  plot(normtemp$hr~normtemp$temp, xlab ="Body Temperature (F)", ylab = "Heart Rate (bpm)")  
  lines(predict(loess_line), col='red', lwd=2)
}
```

From visual inspection of the scatter plot, the points appear to be scattered randomly, but it is possible that there is  a positive relationship between body temperature and heart rate. Based on the loess line, however, the relationship between heart rate and body temperature appears to be random. The line has zero slope, so a linear model should be created to confirm. 

---

### Part 1b

Write the statistical model for estimating heart rate from body temperature, define each term in the model in the context of this application, and write the model assumptions. (Note: the statistical model is the underlying true, but unknown, model for the population that includes the error or noise term.  The model obtained in 1c, is our estimate, obtained using least-squares regression, of the the deterministic (non-random) part of the true model.)

### -|-|-|-|-|-|-|-|-|-|-|- Answer 1b -|-|-|-|-|-|-|-|-|-|-|-

$$\hat{hr} = \hat{\beta_0} + \hat{\beta_1} temp$$
where $temp$ is body temperature in degrees Fahrenheit, $\hat{hr}$ is heart rate in beats per minute, $\hat{\beta_0}$ is the heart rate when body temperature is 0, and $\hat{\beta_1}$ is the rate of increase in heart rate for an increase of 1 degree in body temperature.

Model Assumptions:

1. errors have mean 0
2. errors have the same variance for all $temp$
3. errors are independent of each other
4. errors are normally distributed.


---

### Part 1c  

Obtain the estimated slope and y-intercept for the estimated regression equation and write the equation in the form hr$=\hat{\beta_0} + \hat{\beta_1}temp$ (only with $\hat{\beta_0}$ and $\hat{\beta_1}$ replaced with the numerical estimates from your R output).

### -|-|-|-|-|-|-|-|-|-|-|- Answer 1c -|-|-|-|-|-|-|-|-|-|-|-

```{r}
# Obtain estimated slope and y-intercept.
linear.heartratetemp.model<-with(normtemp,lm(hr~temp))
linear.heartratetemp.model
```

Based on the output from the model, the intercept and coefficient of the estimated regression equation is

$\widehat{\text{hr}}$ = -179.119 + 2.574 * temp  

---

### Part 1d

Test whether or not a positive linear relationship exists between heart rate and body temperature using a 5% level of significance.  State the null and alternative hypotheses, test statistic, the p-value, and conclusion.

### -|-|-|-|-|-|-|-|-|-|-|- Answer 1d -|-|-|-|-|-|-|-|-|-|-|-

```{r}
# Summary of linear model
summary(linear.heartratetemp.model)
```

$H_0 \text{: } \hat{\beta_1}=0$

$H_a \text{: } \hat{\beta_1}>0$

The test statistic is t = 2.878. The p-value is 0.0047. At a 5% level of significance, there is evidence that a positive relationship exists between heart rate and body temperature.

---

### Part 1e

Provide a 95% confidence interval to estimate the slope of the regression equation and interpret the interval in the context of the application (do not us the word “slope” in your interpretation).

### -|-|-|-|-|-|-|-|-|-|-|- Answer 1e -|-|-|-|-|-|-|-|-|-|-|-

```{r}
# 95% confidence interval of rate of change heart rate for 1 degree change in temperature
confint(linear.heartratetemp.model)
```

We are 95% confident that the population mean of heart rate increases 0.80 to 4.34 beats per minute for a one degree increase in temperature.

---

### Part 1f

Provide a 95% confidence interval to estimate the mean heart rate for all adults with body temperature $98.6^o$ F.  Interpret the interval in the context of the problem.

### -|-|-|-|-|-|-|-|-|-|-|- Answer 1f -|-|-|-|-|-|-|-|-|-|-|-

```{r}
# Compute confidence interval
temp_df <- data.frame( temp = 98.6 )
predict(linear.heartratetemp.model, temp_df , interval="confidence")
```

We are 95% confident that, for a body temperature of $98.6^o$ F, the mean heart rate for all adults is between 73.3 and 76.08 beats per minute.

---

### Part 1g
    
Provide a 95% prediction interval to estimate the expected heart rate for a randomly selected adult with body temperature $98.6^o$ F.  Interpret the interval in the context of the problem.
 
### -|-|-|-|-|-|-|-|-|-|-|- Answer 1g -|-|-|-|-|-|-|-|-|-|-|-

```{r}
# Compute prediction interval
temp_df <- data.frame( temp = 98.6 )
predict(linear.heartratetemp.model, temp_df, interval="prediction")
```

We are 95% confident that, for a body temperature of $98.6^o$ F of a randomly selected adult, their heart rate will be between 60.96 and 88.43 beats per minute.

---

### Part 1h

Obtain the residuals and plot them against the predicted values and also against the independent variable.  Also construct a histogram, normal probability plot, and boxplot of the residuals and perform a Shapiro-Wilk test for normality.  Based on your observation of the plot of residuals against the predicted values, does the regression line appear to be a good fit?  Do the model assumptions appear to be satisfied?  Comment. 

### -|-|-|-|-|-|-|-|-|-|-|- Answer 1h -|-|-|-|-|-|-|-|-|-|-|-
```{r include=FALSE}
# Add plotting libraries
require(ggplot2)
require(qqplotr)
require(grid)
require(gridExtra)
```

```{r echo=FALSE}
# Plot resids~pred, resids~temp, resids hist, resids normal prob, and box plot. Perform a Shapiro-Wilk test.
resids <- linear.heartratetemp.model$resid # extract residuals from model
temp <- normtemp$temp #temperature values
fit <- linear.heartratetemp.model$fitted.values #fitted values
resids_df <- data.frame(residuals = resids, pred = fit, temp = temp)
```

```{r echo=FALSE}
resids_pred <- ggplot(resids_df,aes(x=pred, y=residuals)) + geom_point() + geom_hline(yintercept = 0) +
    ggtitle("Residuals against Predicted Values")
resids_pred
#resids_temp <- ggplot(resids_df,aes(x=temp, y=residuals)) + geom_point() 
#grid.arrange(resids_pred, resids_temp,nrow=1, ncol=2, top="Residual Plots")
```


```{r echo=FALSE}
#resids_pred <- ggplot(resids_df,aes(x=pred, y=residuals)) + geom_point() 
resids_temp <- ggplot(resids_df,aes(x=temp, y=residuals)) + geom_point() +
    ggtitle("Residuals against Independent Variable")
resids_temp
#grid.arrange(resids_pred, resids_temp,nrow=1, ncol=2, top="Residual Plots")
```


```{r echo=FALSE, fig.width=4, fig.height=4}
#https://cran.r-project.org/web/packages/qqplotr/vignettes/introduction.html
resid_normal_qq <- ggplot(data = resids_df, mapping = aes(sample = residuals)) +
    stat_qq_band() +
    stat_qq_line() +
    stat_qq_point() +
    labs(x = "Theoretical Quantiles", y = "Sample Quantiles") +
    ggtitle("Normal Q-Q Plot")
resid_normal_qq
#grid.arrange(resid_normal_qq,nrow=1, ncol=1)

```

```{r echo=FALSE}
resids_boxplot <- ggplot(aes(y = residuals), data = resids_df) + geom_boxplot() +
  theme(axis.title.x=element_blank(),axis.text.x=element_blank(),axis.ticks.x=element_blank())
resids_hist <- ggplot(data=resids_df, aes(residuals)) + geom_histogram(breaks=seq(-15, 15, by=1))

grid.arrange(resids_boxplot, resids_hist,nrow=1, ncol=2, top="Boxplot and Histogram of Residuals")
```


```{r echo=FALSE}

#shapiro-wilk test for normality
shapiro.test(resids)

```

```{r echo=TRUE}

#Check that the mean of the residuals is 0.
mean(resids)

```


Based on your observation of the plot of residuals against the predicted values, does the regression line appear to be a good fit?  Do the model assumptions appear to be satisfied?

Model Assumptions:

1. errors have mean 0, look at boxplot, histogram or take mean of residuals.
2. errors have the same variance for all $temp$
3. errors are independent of each other
4. errors are normally distributed.

From the plot of residuals against the predicted values, it can be seen that the residuals are scattered evenly around the zero horizontal line. This suggests that the relationship is linear, and that the errors have the same variance. The errors appear scattered, which would suggest that the errors are independent of each other. The mean of the reisuals is close to zero. Also, based on the sharpiro-wilk test, we fail to reject that the residuals are normally distributed. We can conclude that the regression line does appear to be a good fit and that the model assumptions are satisfied.

---

### Part 1i

Examine the original scatterplot and the residual plot. Do any observations appear to be influential or be high leverage points?  If so, describe them and what effect they appear to be having on the estimated regression equation.

### -|-|-|-|-|-|-|-|-|-|-|- Answer 1i -|-|-|-|-|-|-|-|-|-|-|-

There are high and low points in the middle. I think that there strong influence in high and low direction cancels out an effect on the estimated regression equation. However, due to greater variability, it may have some influence on the confidence interval. 

---

### Part 1j

Perform the F test to determine whether there is lack of fit in the linear regression function for predicting heart rate from body temperature.  Use $\alpha = 0.05$.  State the null and alternative hypotheses, test statistic, the p-value, and the conclusion.

### -|-|-|-|-|-|-|-|-|-|-|- Answer 1j -|-|-|-|-|-|-|-|-|-|-|-

```{r}
# Create full model.
heartratetemp.model.full<-with(normtemp,lm(hr~factor(temp)))
results <- anova( linear.heartratetemp.model, heartratetemp.model.full )
#results$F
#results$'Pr(>F)'
results
```

$$ H_0: \mbox{line model}, \quad H_a: \mbox{full model} $$

The F-statistic 1.403484 and the p-value is 0.1103044. At a 5% level of significance, we fail to reject the null hypothesis that the line model is a good fit for the data.

---

### Part 1k

Conduct the Breusch-Pagan test for the constancy of error variance.  Use α = 0.05.  State the null and alternative hypotheses, test statistic, the P-value, and the conclusion.

### -|-|-|-|-|-|-|-|-|-|-|- Answer 1k -|-|-|-|-|-|-|-|-|-|-|-

```{r include=FALSE}
# Breusch-Pagan test for the constancy of error variance
require(lmtest) # install if needed
```

```{r}
# Breusch-Pagan test for the constancy of error variance
bptest(linear.heartratetemp.model)
```

$$ H_0: \mbox{equal variances}, \quad H_1: \mbox{unequal variances}$$
The BP statistic is 0.19584 and the p-value is 0.6581. At a 5% level of significance, we fail to reject the null hypothesis that the model has equal variances.

---

### Part 1l

Calculate and interpret the Pearson correlation coefficient $r$.

### -|-|-|-|-|-|-|-|-|-|-|- Answer 1l -|-|-|-|-|-|-|-|-|-|-|-

```{r}
# Calculate and interpret Pearson correlation coefficient
correlation.test <- with(normtemp, cor.test(temp, hr, method="pearson") )
correlation.test$estimate
```

The pearson correlation coefficient of .25 indicates a weak positive relationship between body temperature and heart rate.

---

### Part 1m

Construct a 95% confidence interval for the Pearson correlation coefficient $r$.

### -|-|-|-|-|-|-|-|-|-|-|- Answer 1m -|-|-|-|-|-|-|-|-|-|-|-

```{r}
# Construct a 95% CI for the Pearson correlation coefficient.
correlation.test$conf.int

```

---

### Part 1n

Calculate and interpret the coefficient of determination $r^2_{yx}$ (same as $R^2$).

### -|-|-|-|-|-|-|-|-|-|-|- Answer 1n -|-|-|-|-|-|-|-|-|-|-|-

```{r include=FALSE}
# Calculate and interpret the coefficient of determination.
summary(linear.heartratetemp.model)

```

The adjusted R-squared is 0.05424. That means only 5.4% of the variation is explained by the model. 

---

### Part 1o

Should the regression equation obtained for heart rate and temperature be used for making predictions?  Explain your answer.

### -|-|-|-|-|-|-|-|-|-|-|- Answer 1o -|-|-|-|-|-|-|-|-|-|-|-

Based on the adjusted R squared, the model does not have signficant predictive powers, and probably should not be used to predict heart rate. However, there is some explanatory power in temperature to heart rate, and the model could be updated to include more factors like age for example in order to make better predictions.

---

### Part 1p

Calculate the Spearman correlation coefficient $r_s$ (just for practice).

### -|-|-|-|-|-|-|-|-|-|-|- Answer 1p -|-|-|-|-|-|-|-|-|-|-|-

```{r echo=FALSE}
# Calculate the Spearman correlation coefficient.
correlation.test <- with(normtemp, cor.test(temp, hr, method = "spearman") )
correlation.test$estimate

```

---

### Part 1q

Create 95% prediction and confidence limits for the predicted mean heartrate for each temperature given in the sample data and plot them along with a scatterplot of the data. (Look for the slides titled "Confidence Bands" in the presentation.)

```{r echo = FALSE}
# 95% prediction and confidence limits.
xplot <- data.frame( NumItems = seq( 95, 100, length=128) )
fittedC <- predict(linear.heartratetemp.model,xplot,interval="confidence")
fittedP <- predict(linear.heartratetemp.model,xplot,interval="prediction")

# scatterplot
ylimits <- c(min(fittedP[,"lwr"]),max(fittedP[,"upr"]))
{
  plot(normtemp$temp,normtemp$hr,ylim=ylimits, xlab ="Body Temperature (F)", ylab = "Heart Rate (bpm)")
  abline(linear.heartratetemp.model)
  # plot the confidence and prediction  bands
  lines(xplot[,1], fittedC[, "lwr"], lty = "dashed",col='darkgreen')
  lines(xplot[,1], fittedC[, "upr"], lty = "dashed",col='darkgreen')
  lines(xplot[,1], fittedP[, "lwr"], lty = "dotted",col='blue')
  lines(xplot[,1], fittedP[, "upr"], lty = "dotted",col='blue')
}             

```

---

## Exercise 2

A personnel officer in a governmental agency administered three newly developed aptitude tests to a random sample of 25 applicants for entry-level positions in the agency.  For the purpose of the study, all 25 applicants were accepted for positions irrespective of their test scores.  After a probationary period, each applicant was rated for proficiency on the job.  

The scores on the three tests (x1, x2, x3) and the job proficiency score (y) for the 25 employees are in the file JobProf.rda.

(Based on an exercise from Applied Linear Statistical Models, 5th ed. by Kutner, Nachtsheim, Neter, & Li)

### Part 2a

We'd like to explore using interaction terms in a statistical model 
including the three first-order terms and the three cross-product interaction terms:

$$y=\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_1 x_2 + \beta_5 x_1 x_3 + \beta_6 x_2 x_3 + \epsilon$$

Use R to find the corresponding estimated model and also obtain the `summary()`.

## -|-|-|-|-|-|-|-|-|-|-|- Answer 2a -|-|-|-|-|-|-|-|-|-|-|-

```{r}
# Get the data
data("JobProf")
linear.model <- lm(y~x1 + x2 + x3 + x1:x2 + x1:x3 + x2:x3, data=JobProf)
summary(linear.model)
```

---

### Part 2b

Use R to compute the VIF for each term in the model.  Are any of the VIFs over 10?  (We need to add this into Lesson 6, but it's covered in the Lesson 8 Swirl - I've put an example in the chunk below.  Replace the chunk with code to find the VIF's for this model.)

## -|-|-|-|-|-|-|-|-|-|-|- Answer 2b -|-|-|-|-|-|-|-|-|-|-|-

```{r include=FALSE}
# Change this code so it computes VIF's for the model in the problem
# if necessary install.packages('HH'), but do it in the console and reply 'n'
# if it asks if you want to compile a binary package
# in this case the low VIF's indicate collinearity is not a problem for the terms
require(HH)
#require(DS705data)
#data(HealthExam)
#fit <- lm(Weight~Height+Waist+SysBP,HealthExam)
#vif(linear.model)
```

```{r}
vif(linear.model)
```
All of the VIFs are greater than 10. There is a high degree of collinearity.

--- 

### Part 2c

The model from 2a is suffering from the effects of collinearity (which you should see in 2b), which inflates the standard errors of the estimated coefficients.

Using the model summary from 2a what do you notice about the overall model p-value (from the F-statistic) and the individual p-values for each term in the model?  Does it make sense that the overall model shows statistical significance but no individual term does?  

### -|-|-|-|-|-|-|-|-|-|-|- Answer 2c -|-|-|-|-|-|-|-|-|-|-|-

The overall model p-value from the F-statistic is 4.042e-10 which indicates a very good fit, but the individual p-values for each term is large indicating that none of the individual terms are signficant. The small p-value on the F statistic implies overall the predictors together are significant in explaining the variation in the predictor variable. The high p-values on individual terms are caused by the high degree of collinearity between the terms. Due to the high degree of collinearity and after accounting for the other variables, adding a new variable does not explain much more of the variation, and thus has a high p-value. 

---

### Part 2d

Use R to estimate and `summarize()` the first order model corresponding to 

$$y=\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \epsilon$$

Is the first order model significant?

### -|-|-|-|-|-|-|-|-|-|-|- Answer 2d -|-|-|-|-|-|-|-|-|-|-|-

```{r}
# Estimate and summarize a first order model.
linear.model.witoutinteractionterms <- lm(y~x1 + x2 + x3, data=JobProf)
summary(linear.model.witoutinteractionterms)
```

This model is more significant since it has a smaller p-value in the F-statistic, and two out of the three terms are now statistically significant.

---

### Part 2e

Do the interaction terms in 2a really add anything significant beyond the first order model in 2d?  Now we'll compare the models with and without interaction terms to see if the interaction terms make a statistically significant improvement to the fit of our models.

Test the significance of all three coefficients for the interaction terms as a subset by using `anova()` to compare the model from Part 2a to the first order model from Part 2d. Use a 5% level of significance.  State $H_0$ and $H_a$ and provide the R output as well as a written conclusion which includes the P-value.  Should we keep the interaction terms?

### -|-|-|-|-|-|-|-|-|-|-|- Answer 2e -|-|-|-|-|-|-|-|-|-|-|-

```{r}
#Compare the models
results <- anova( linear.model, linear.model.witoutinteractionterms )
results
```

$$ H_0: \mbox{interaction model}, \quad H_a: \mbox{no interaction model} $$

The F-statistic 0.7444 and the p-value is 0.5395. At a 5% level of significance, we fail to reject the null hypothesis that the model with interaction terms is a good fit for the data.


---

### Part 2f

There are more methodical approaches to exploring different models that we'll learn about in a later lesson, but we'll try one more model here to get a bit more experience.  In this case we'll add a quadratic term $x_2^2$.  To do this you'll want to create a new variable `x2sq = x2^2` and include it in your model.  Use R to estimate and `summarize()` the model corresponding to: 

$$y=\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_2^2 +\epsilon$$

Examine the p-value corresponding to the quadratic term.  If the quadratic term is significant at significance level $\alpha = 0.05$, then according to the hierarchical approach we should retain it and the $x_2$ term.  If it isn't significant, then we won't retain it but we'll have to evaluate the significance of the $x_2$ term separately.

Should the quadratic term be retained in the model at a 5% level of significance?  

### -|-|-|-|-|-|-|-|-|-|-|- Answer 2f -|-|-|-|-|-|-|-|-|-|-|-

```{r}
# Add quadratic term
x2sq =JobProf$x2^2
linear.model.with.xsquared <- lm(y~x1 + x2 + x3 + x2sq, data=JobProf)
summary(linear.model.with.xsquared)
```

The p-value for the x2 squared term is 0.353. At at 5% level of signficance, it should not be retained.

---

### Part 2g

If you've been successful so far, then you should realize that the none of interaction terms nor the quadratic term have been significant (if you concluded otherwise, then review your work). This brings us back to the first order model in Part 2d.  Look at that model summary again.  There should be one term that is insignificant so omit it and use R to estimate our final and smaller first order model.

### -|-|-|-|-|-|-|-|-|-|-|- Answer 2g -|-|-|-|-|-|-|-|-|-|-|-

```{r}
# Estimate final and smaller model
linear.model.final.first.order <- lm(y~x1 + x3, data=JobProf)
summary(linear.model.final.first.order)

```

---

### Part 2h

From the final first order model in 2g, obtain a 90% confidence interval for the coefficient of $x_3$ and interpret it in the context of this problem.

### -|-|-|-|-|-|-|-|-|-|-|- Answer 2g -|-|-|-|-|-|-|-|-|-|-|-

```{r}
# 90% confidence interval
confint(linear.model.final.first.order, level = .9)
```

A 90% confidence interval for x3 is [1.61, 2.03]. One additional point on the third aptitude test corresponds to an increase in between 1.61 and 2.03 points in the job proficiency score.     

---

### Part 2i

Using the final first order model from 2g, construct a 95% prediction interval for a randomly selected employee with aptitude scores of $x_1=99, x_2=112,$ and $x_3=105$ to forecast their proficiency rating at the end of the probationary period. Write an interpretation for the interval in the context of this problem.

### -|-|-|-|-|-|-|-|-|-|-|- Answer 2i -|-|-|-|-|-|-|-|-|-|-|-

```{r}
# Compute prediction interval
temp_df <- data.frame( x1=99,x2=112,x3=105 )
predict(linear.model.final.first.order, temp_df, interval="prediction")

```

With 95% confidence, a randomly selected employee who has a score of 99 on test 1, 112 on test 2, and 105 on test 3 on the aptitude tests will have a job proficiency score of between 87 and 110.

---

## Exercise 3

Consider the scenario from Exercises 12.5 and 12.7 on page 725 of Ott's textbook.  There are two categorical variables (Method and Gender) and one quantitative variable (index of English proficiency prior to the program).  See the textbook for details on how the qualitative variables are coded using indicator variables.

### Part 3a

Use data in the file English.rda in the DS705data package to estimate the coefficients for the model in Exercise 12.5:

$$y=\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon$$ 

Obtain the estimated intercept and coefficients and state the estimated mean English proficiency scores for each of the 3 methods of teaching English as a second language.

### -|-|-|-|-|-|-|-|-|-|-|- Answer 3a -|-|-|-|-|-|-|-|-|-|-|-

```{r}
# Get the data and estimate coefficients.
data("English")
model <- lm(y~x1 + x2, data=English)
summary(model)
```

The intercept is 44.75. x1 is 61.4. x2 is 3.95.
The estimated mean English proficiency score for method 1 is 61.4 + 44.75 = 106.15, method 2 is 3.95 + 44.75 = 48.7, and method 3 is just the value of the intercept 44.75.

---

### Part 3b  

Fit the model for Exercise 12.7:

$$y=\beta_0 + \beta_1 x_4 + \beta_2 x_1 + \beta_3 x_2 + \beta_5 x_1 x_4 + \beta_6 x_2 x_4 + \epsilon$$

Using the estimated coefficients, write three separate estimated models, one for each method, relating the scores after 3 months in the program (y) to the index score prior to starting the program ($x_4$).

### -|-|-|-|-|-|-|-|-|-|-|- Answer 3b -|-|-|-|-|-|-|-|-|-|-|-

```{r}
# Fit model
model <- lm(y~x4 + x1 + x2 + x1:x4 + x2:x4, data=English)
summary(model)

```

Model 1:

$$y=39.2585 + 0.1220 x_4 - 20.3014 x_1 + 1.7797 x_1 x_4 + \epsilon$$

Model 2:

$$y=39.2585 + 0.1220 x_4 - 9.4661 x_2 + 0.3038 x_2 x_4 + \epsilon$$
Model 3:

$$y=39.2585 + 0.1220 x_4 + \epsilon$$